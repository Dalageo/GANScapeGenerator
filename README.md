<div align="center">
  <img src="https://github.com/user-attachments/assets/42d403fc-4406-436e-883d-f4e5dd61cc39" alt="Landscape-Gif">
</div>

<div align="center">
  <a href="https://www.python.org/downloads/release/python-3920/" target="_blank">
  <img src="https://img.shields.io/badge/Python-3.9.20-blue.svg" alt="Python 3.9.20"></a>
  <a href="https://pytorch.org/get-started/locally/" target="_blank">
    <img src="https://img.shields.io/badge/PyTorch-2.5.1-orange.svg" alt="PyTorch 2.5.1"></a>
  <a href="https://developer.nvidia.com/cuda-11-8-0-download-archive" target="_blank">
  <img src="https://img.shields.io/badge/CUDA-11.8-brightgreen.svg" alt="CUDA 11.8"></a>
<a href="https://developer.nvidia.com/cudnn" target="_blank">
  <img src="https://img.shields.io/badge/cuDNN-9.1.0-brightgreen.svg" alt="cuDNN 9.1.0"></a>
  <a href="https://github.com/Dalageo/GANScapeGenerator/blob/main/LICENSE" target="_blank">
    <img src="https://img.shields.io/badge/License-AGPL%20v3-800080" alt="License: AGPLv3"></a>
  <img src="https://img.shields.io/github/stars/Dalageo/GANScapeGenerator?style=social" alt="GitHub stars">
</div>

# Generating Landscapes Using DCGAN and StyleGAN3 üèûÔ∏è

This project examines image generative models, focusing on DCGAN and StyleGAN3. Initially, the project started by training a DCGAN to generate landscape images, but it was found that to create a decent DCGAN, you need a significantly larger and more diverse dataset, a carefully designed architecture, advanced techniques to stabilize training between the generator and discriminator, and sufficient computational resources to train effectively. As a result, the project did not delve deeply into developing the most powerful DCGAN architecture. Instead, it explored the use of a pretrained variant of [StyleGAN3](https://github.com/NVlabs/stylegan3), originally provided by [NVIDIA](https://www.nvidia.com/en-eu/) and further fine-tuned on landscape images by [Justin Pinkney](https://www.justinpinkney.com/).

More specifically, the model used is the *[StyleGAN3-t LHQ 256](https://huggingface.co/justinpinkney/stylegan3-t-lhq-256)*, which is a StyleGAN3-t model further trained on 15 million images of various landscapes at a resolution of 256x256. Examples of initial generated outputs from this model are shown in the following stacked arrangement of three images:

<div align="center">
 <img src="https://github.com/user-attachments/assets/07b0cf89-2289-4c57-9d1b-7061122326ff" alt="LHQExample">
</div>

*Further fine-tuning* of this model was carried out for 50 epochs using the [Landscape Pictures](https://www.kaggle.com/datasets/arnaud58/landscape-pictures/data) dataset, with the same seeds used to generate new landscape images after this additional training, which are displayed below. By comparing the two stacks of images, it is evident that the new dataset likely introduced greater colors and lighting variations, increased detail, and possibly more geographical diversity, since the landscapes in the second stack feature richer and more complex environments, ranging from detailed mountain terrains to lush, vibrant valleys.

<div align="center">
  <img src="https://github.com/user-attachments/assets/cb9a4333-92f7-4e5a-8ab8-79cea98acc4d" alt="FineTunedExample">
  <br>
</div>

Additional generated images are available in a GIF at the top of this repository, with the top three images generated by the StyleGAN3-t LHQ 256 and the bottom three by the further trained model. Below is a GIF of images generated by the provided *DCGAN architecture*: 

<div align="center">
  <img src="https://github.com/user-attachments/assets/51506540-8b6e-460c-a468-d456c7326472" alt="DCGANExample">
</div>

These images demonstrate the architecture's ability to generate landscape-like visuals, but with noticeable limitations such as lower resolution, simplified color schemes, and less realistic textures compared to the more advanced StyleGAN3 outputs. This difference highlights the benefits of using pre-trained models, especially when training resources are limited.

## Dataset Description

The [Landscape Pictures](https://www.kaggle.com/datasets/arnaud58/landscape-pictures/data) dataset, a collection of natural landscape photos from [Flickr](https://www.flickr.com/), was used to train both DCGAN and StyleGAN3-t LHQ 256 models. It consists of 4,300 images, representing a variety of landscape types. Details of these categories, including the number of pictures and a brief description of each, are provided in the table below:

| Landscape Category           | Number of Pictures | Description                              |
|-----------------------|--------------------|------------------------------------------|
| `landscapes`          | 900                | General landscape pictures               |
| `landscapes_mountain` | 900                | Pictures featuring mountain landscapes   |
| `landscapes_desert`   | 100                | Pictures of desert landscapes            |
| `landscapes_sea`      | 500                | Sea views and coastal landscapes         |
| `landscapes_beach`    | 500                | Beach scenes                             |
| `landscapes_island`   | 500                | Pictures of island settings              |
| `landscapes_japan`    | 900                | Landscapes located in Japan              |

*To adapt to my system's and model capabilities, the landscape images, which originally varied in resolution, were uniformly resized to a resolution of 256x256 pixels.*

## Setup Instructions

### <img src="https://github.com/user-attachments/assets/8d36d1a5-e9b1-40d1-97c9-3d4ca49e9c95" alt="Local PC" width="18" height = "16" /> **Local Environment Setup**

1. **Clone the repository**:
   ```sh
   git clone https://github.com/Dalageo/GANScapeGenerator.git

2. **Navigate to the cloned directory**:
   ```sh
   cd GANScapeGenerator

### For DCGAN:
3. **Open the `GANScapeGenerator_DCGAN.ipynb` using your preferred Jupyter-compatible environment. (e.g., [Jupyter Notebook](https://jupyter.org/), [VS Code](https://code.visualstudio.com/), or [PyCharm](https://www.jetbrains.com/pycharm/))**
   
4. **Update the dataset, model and output directory paths to point to the location of your local environment.**
   
5. **Run the cells sequentially to reproduce the results.**

### For StyleGan3:
3. **Visit the [StyleGAN3 repository](https://github.com/NVlabs/stylegan3) and follow the installation instructions.**

4. **Open the `GANScapeGenerator_StyleGAN3.py` using your preferred Python-compatible environment (e.g., [VS Code](https://code.visualstudio.com/), or [PyCharm](https://www.jetbrains.com/pycharm/))**

5. **Select a pretrained StyleGAN3 model suitable for your dataset:(e.g., [NVIDIA's NGC catalog](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/research/models/stylegan3/files) or Finetuned stylegan3 models on [Hugging Face](https://huggingface.co/models?search=stylegan-3))**
   
6. **Update the dataset, model and output directory paths**

7. **Run the cells sequentially to reproduce the results.**

*To train the models on GPU, you will need to activate GPU support based on your operating system and install the required dependencies. You can follow this [guide](https://pytorch.org/get-started/locally/) provided by [PyTorch](https://pytorch.org/) for detailed instructions.*

## Acknowledgments
 
Firstly, I would like to thank Alec Radford, Luke Metz, and Soumith Chintala for introducing DCGAN in their 2015 paper, *["Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"](https://arxiv.org/abs/1511.06434)*. 

Additionally, special thanks to [NVIDIA](https://www.nvidia.com) for providing pretrained StyleGAN3 models for educational and research purposes, as well as [Justin Pinkney](https://www.justinpinkney.com/) for making available a StyleGAN3 variant that has been pretrained on the LHQ dataset.

<div align="center">
  <br>
  <a href="https://www.nvidia.com">
    <img src="https://github.com/user-attachments/assets/9e708a82-d6e7-4454-b839-23650dd969de" alt="Nvidia Logo" width="150"/></a>
</div>

## License
The provided fine-tuned StyleGAN3 model is licensed under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt), the dataset is under the [CC 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/), while the accompanying documentation is licensed under the [AGPL-3.0 license](https://www.gnu.org/licenses/agpl-3.0.en.html). AGPL-3.0 license was chosen to promote open collaboration, ensure transparency, and allow others to freely use, modify, and contribute to the work. 

Any modifications or improvements must also be shared under the same license, with appropriate acknowledgment. 

<div align="center">
  <br>
  <a href="https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt" style="margin: 0 10px;">
    <img src=https://github.com/user-attachments/assets/eb00a939-9cee-421d-bf65-784643233f36 alt="Nvidia-Logo" width="100"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://creativecommons.org/publicdomain/zero/1.0/deed.en" style="margin: 0 10px;">
    <img src=https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg alt="CC 1.0 Universal" width="150"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://github.com/user-attachments/assets/f3c6face-aa86-45da-8d20-d8ae25e49e28" alt="AGPLv3-Logo" width="200""></a>
</div>




